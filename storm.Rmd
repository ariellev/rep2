---
title: "Severe Weather Events in the US between 1950 and 2011: Costs in Property and Publich Health"
output: 
  html_document:
    keep_md: true
---

###Synopsis

This report is based on a NOAA Storm Dataset containing over 900K meteorological events made across the U.S. between 1950 and 2011. The meteorolgical events were categorized into 48 different types specified at **Table 1: Storm Data** in [National Weather Service Instruction 10-1605.pdf](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf) . The focus of my study was to assess the damage made to both property and publich health, categorized by each and every event type. The events with the most mortality rates were X,Y,Z. The study of the correlation between injury and fatality rate is an ineresting question but unfortunately will not be covered by the scope of this report. Among the most destructive events in terms of loss of property and corps are X,Y,Z with averagely more than Billions USD. 

###Data Processing
```{r,echo=T, cache=T}
sessionInfo()
```

Loading required packages.
```{r,echo=T, cache=T, message = F}
# detaching plyr to avoid conflicts with dplyr
detach(package:plyr)
require(dplyr)
require(ggplot2)
require(stringr)
```

Loading the dataset into R
```{r,echo=T, cache=T}
# loading the dataset into R
d <- read.csv(bzfile("storm.bz2"), stringsAsFactors=F)

# dataset was downloaded from: "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
# dataset was downloaded on:   "2015-04-18 07:37:06 CEST"

# exploring dimensions and variables
str(d)
```

The to the loss assement essential variables are listed in the table below. The rest of the variables can be ignored for the scope of our study - making our dataset more lightweight to work with. 

| variable        | description                 |
|:----------------|:----------------------------|
| EVTYPE          | event type                  |
| FATALITIES      | total fatalities            |
| INJURIES        | total injuries              |
| PROPDMG         | property damage - mantissa  |
| PROPDMGEXP      | property damage - exponent  |
| CROPDMG         | crop damage - mantissa      |
| CROPDMGEXP      | crop damage - exponent      |
| REMARKS         | additional information*     | 

*will be used for cases where EVTYPE are of a form of "Summary of.." 
```{r,echo=T, cache=T}
# projecting dataset
cols <- c(8,23:28,36)
d <- d[,cols]
names(d)
```

The examination of the size of the unique values of EVTYPE yields a fairly bigger figure than the 48 permitted events by NOAA. We will therefore have to tidy up ETYPES and bring it to a more consistent state.
```{r,echo=T, cache=T}
e_types <- d$EVTYPE
length(unique(e_types))
```

In the next section I would like to demonstrate a method to clean EVTYPE. The strategy would be in general to **automatically** find a best match between a given EVTYPE to a probable candidate in **Table 1**. To find such a match I will introduce a simple utility function written specficily for that purpose. Prior to finding a best match, we will have to reduce the noise, i.e the modficiations of simliar words to the ones in **Table 1**, and the complete removal of foreign. For that I will apply some simple techniches of string manipulations, such as replacement of non alphabetical character, and introduce a second utilty function that utilizes the filtering. My objective was to fully automate this process. However I could not avoid using one single manual step: copying the table from the pdf into a text file format. 


#####Copy Table 1
goto [National Weather Service Instruction 10-1605.pdf](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf) and scroll down to page 6 to Table 1 *Storm Data*, mark the table with the mouse, copy the content to a clipboard, create a text file called *"noaa_e_types.txt"*, and paste the clipboard into it.
```{r,echo=T, cache=T}
```
  
```{r,echo=T, cache=T}

noaa_glossary <- scan("noaa_e_types.txt", what = "characters")
noaa_glossary <- unique(noaa_glossary)
noaa_glossary <- unlist(strsplit(noaa_glossary,"[^a-zA-Z]", fixed=F))
noaa_glossary <- noaa_glossary[nchar(noaa_glossary) > 1]
noaa_glossary <- toupper(noaa_glossary)
noaa_glossary <- noaa_glossary[order(noaa_glossary)]
noaa_glossary

# declaring a utility function 
noaa_filter <- function(s_vec, dictionary, replacement = "*") {
  r_vec <- c()
  for (s in s_vec) {
  	token <- ""
		split <- unlist(strsplit(s, " "))
		for (w in split) {
			if (w %in% dictionary) {
			  token <- paste(token, w)
			} else {
 			  token <- paste(token, replacement)
			}
		}
		token <- gsub("[ ]+", " ",  token)
		token <- str_trim(token)
		r_vec <- append(r_vec, token)
	}
	r_vec
}

# usage example
eu <- unique(e_types)
data.frame(before = eu[658:660], after = noaa_filter(eu[658:660], noaa_glossary, "(*)"))
```
Handling EVTYPE of the form of "Summary of .."
The information describing these type of events is given as a free text under the REMARKS variable. A propert way threfore to handle the cleanup in these situations would be to anlayze the REMARKS variable, understanding its context, and finding it a suitable match in **Table 1**. However, after examining the rate of such an occurance, we can see that it is noticeably low. For the sake of ease of calculation, I would assign the type "" 
```{r,echo=T, cache=T}
# example
head(d[grepl(".*summary.*", d[,1], ignore.case = T), c(1,8)],1)

# ratio 
e_summary <- grepl(".*summary.*", d[,1], ignore.case = T)
data.table( mean = mean(e_summary), total = sum(e_summary))
```

```{r,echo=T, cache=T}
# applying string manipulation to clean the EVTYPE variable
# upper Case
e_types <- toupper(e_types)
# removing non alphabetical chars
e_types <- gsub("[^A-Z ]", " ",  e_types)
# removing spaces
e_types <- gsub("[ ]+", " ",  e_types)
# trimming
e_types <- str_trim(e_types)

length(unique(e_types))
```

##### Modifying similar words
```{r example,echo=T, cache=T}
u <- unique(e_types)
u[grepl("SNOW[A-Z]+", u)]
```

```{r,echo=T, cache=T}
e_types <- gsub("[A-Z]*FLOOD[A-Z]*", " FLOOD ", e_types)
e_types <- gsub("[A-Z]*SNOW[A-Z]*", " SNOW ", e_types)
e_types <- gsub("[A-Z]*WILDFIRES[A-Z]*", " WILDFIRES ", e_types)
e_types <- gsub("[A-Z]*TSTM[A-Z]*", " THUNDERSTORM ", e_types)
length(unique(e_types))
```



NOAA noaa_glossary.
Example on non existing term
```{r,echo=T, cache=T}
# TODO
```


```{r,echo=T, cache=T}
df0 <- data.frame(e_type=eu, e_filtered = noaa_filter(eu, noaa_glossary, ""))
eu <- unique(df0$e_filtered)
length(eu)
```

finding a match
example
```{r,echo=T, cache=T}
# TODO
```

```{r,echo=T, cache=T}
noaa_e_types <- scan("noaa_e_types.txt", what = "characters", sep="\n")
noaa_e_types <- substr(noaa_e_types, 0, nchar(noaa_e_types) - 1)
noaa_e_types <- toupper(noaa_e_types)
noaa_e_types <- gsub("[^A-Z ]", " ",  noaa_e_types)
noaa_e_types <- gsub("[ ]+", " ",  noaa_e_types)
noaa_e_types <- str_trim(noaa_e_types)
noaa_e_types

# declaring a utility function 
noaa_match <- function(s_vec, with_vec) {
  r_vec <- c()
	for (s in s_vec) {
		split_s <- unlist(strsplit(s, " "))
		found_total <- 0
		found_word <- ""
		for ( w in with_vec) {
		  split_w <- unlist(strsplit(w, " "))
		  inter <- length(intersect(split_s, split_w)) / length(union(split_s, split_w)) 
		  if (inter > found_total) {
			found_total <- inter
			found_word <- w
		  }
		}
		r_vec <- append(r_vec, found_word)
	}
	r_vec
}

# usage example
data.frame(event = eu[6:9], match = noaa_match(eu[6:9], noaa_e_types))

df1 <- data.frame(e_filtered=eu, bool=eu %in% noaa_e_types)
df1$e_match <- noaa_match(eu, noaa_e_types)
df1[!df1$bool,c(1,3)][148:155,]
```

To complete the phase, lets merge df0, df1 and e_types
```{r,echo=T, cache=T}
# joining df0 and df1
df_0_1 <- left_join(df0,df1)
head(df_0_1)
df_0_1 <- df_0_1[,c(1,4)]
head(df_0_1)

# joining e_types and df_0_1
e_joined <- left_join(data.frame(e_type=e_types),df_0_1)
sample_n(e_joined, 10)
e_types <- e_joined$e_match
d[,1] <- e_types

# excluding empty observations 
d <- d[nchar(d$EVTYPE) > 0,]
```


